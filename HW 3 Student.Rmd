---
title: "HW 3"
author: "Collin Register"
date: "11/27/2023"
output: 
  html_document:
    number_sections: true
---

# 

In this homework, we will discuss support vector machines and tree-based methods.  I will begin by simulating some data for you to use with SVM. 

```{r}
library(e1071)
set.seed(1) 
x=matrix(rnorm(200*2),ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot(x, col=y)

```


##

Quite clearly, the above data is not linearly separable.  Create a training-testing partition with 100 random observations in the training partition.  Fit an svm on this training data using the radial kernel, and tuning parameters $\gamma=1$, cost $=1$.  Plot the svm on the training data.  

```{r}
# Create training-testing partition
ran <- sample(1:nrow(dat), 100)
train <- dat[ran, ]
test <- dat[-ran, ]

svmfit <- svm(y ~ ., data=train, kernel="radial", gamma=1, cost=1)
print(svmfit)

plot(svmfit, train)


```


##

Notice that the above decision boundary is decidedly non-linear.  It seems to perform reasonably well, but there are indeed some misclassifications.  Let's see if increasing the cost ^[Remember this is a parameter that decides how smooth your decision boundary should be] helps our classification error rate.  Refit the svm with the radial kernel, $\gamma=1$, and a cost of 10000.  Plot this svm on the training data. 

```{r}
svmfit2 <- svm(y ~ ., data=train, kernel="radial", gamma=1, cost=10000)
print(svmfit2)

plot(svmfit2, train)

```

##

It would appear that we are better capturing the training data, but comment on the dangers (if any exist), of such a model. 

Dangers of such a model exist such as trying to make something linear that is not linear along with issues of overfitting. By doing this radial function with a high cost we are no longer able to generalzie it, instead it is just fitting the specific data. Another danger is that we want the training and testing data to be similar and don't want it to be too complex.

##

Create a confusion matrix by using this svm to predict on the current testing partition.  Comment on the confusion matrix.  Is there any disparity in our classification results?    

```{r}
#remove eval = FALSE in above
table(true=dat[-ran,"y"], pred=predict(svmfit2, newdata=dat[-ran,]))

```
Given the confusion matrix, we have 57 correct classifications and 20 wrong classifications of class 1. Of class 2 we have 5 wrongly classified and 18 correctly classified. Yes there is disparity between 20 and 5 for the two class types.


##

Is this disparity because of imbalance in the training/testing partition?  Find the proportion of class `2` in your training partition and see if it is broadly representative of the underlying 25\% of class 2 in the data as a whole.  

```{r}


train_prop <- sum(train$y==2)/100
print(train_prop) 

```

The disparity is only by 2 observations or a proportion difference of 0.02, where the training partition is not representative of the class 2 in the whole of the data. Overall this is a small disparity and would not account for the large disparity found in the confusion matrix.

##

Let's try and balance the above to solutions via cross-validation.  Using the `tune` function, pass in the training data, and a list of the following cost and $\gamma$ values: {0.1, 1, 10, 100, 1000} and {0.5, 1,2,3,4}.  Save the output of this function in a variable called `tune.out`.  

```{r}
set.seed(1)
tune.out <- tune(svm,y ~ ., data = train, kernel = "radial", cost = c(0.1, 1, 10, 100, 1000), gamma = c(0.5, 1, 2, 3, 4))
print(tune.out)

```

I will take `tune.out` and use the best model according to error rate to test on our data.  I will report a confusion matrix corresponding to the 100 predictions.  


```{r}
table(true=dat[-ran,"y"], pred=predict(tune.out$best.model, newdata=dat[-ran,]))

```

##

Comment on the confusion matrix.  How have we improved upon the model in question 2 and what qualifications are still necessary for this improved model.  

The new confusion matrix has improved the number of true positives for class 1 and reduced misclassifcations for type 1 as type 2. The improved model still needs to better classify type 2 as correct classification on class 2 were worse. 

# 
Let's turn now to decision trees.  

```{r}
library(kmed)
data(heart)
library(tree)
```

## 

The response variable is currently a categorical variable with four levels.  Convert heart disease into binary categorical variable.  Then, ensure that it is properly stored as a factor. 

```{r}

Absent<- ifelse(heart$class <= 0, 'Absent', 'Present')
heart <- data.frame(heart, Absent)
Absentfac<-as.factor(Absent)
heart<-data.frame(heart, Absentfac)
heart<-heart[,-15]
str(heart)

```

## 

Train a classification tree on a 240 observation training subset (using the seed I have set for you).  Plot the tree.  

```{r}
set.seed(101)
train=sample(1:nrow(heart), 240)

tree.heart = tree(Absentfac~.-class, heart, subset=train)
plot(tree.heart)
text(tree.heart, pretty=0)

```


## 

Use the trained model to classify the remaining testing points.  Create a confusion matrix to evaluate performance.  Report the classification error rate.  

```{r}
tree.pred = predict(tree.heart, heart[-train,], type="class")
with(heart[-train,], table(tree.pred, class))
#accuracy
(28+18)/(28+2+1+8+5+5+5+3)
#classification error rate
(2+1+8)/(28+2+1+8+5+5+5+3)



```

##  

Above we have a fully grown (bushy) tree.  Now, cross validate it using the `cv.tree` command.  Specify cross validation to be done according to the misclassification rate.  Choose an ideal number of splits, and plot this tree.  Finally, use this pruned tree to test on the testing set.  Report a confusion matrix and the misclassification rate.  

```{r}


cv.heart=cv.tree(tree.heart, FUN=prune.misclass)
cv.heart

plot(cv.heart$size,cv.heart$dev,type="b")


prune.heart=prune.misclass(tree.heart, best=3)
plot(prune.heart)
text(prune.heart, pretty=0)

tree.pred2 = predict(prune.heart, heart[-train,], type="class")
with(heart[-train,], table(tree.pred2, Absentfac))
#accuracy
(23+21) / (23+21+9+4)
#classification error rate
(9+4) / (23+21+9+4)
```


##

Discuss the trade-off in accuracy and interpretability in pruning the above tree. 

Here the tradeoff in accuracy and interpretability is that our classification error rate increases by about 2 percent but our tree is much easier to understand. The less complicated tree is easily interpreted, while only losing a small amount of accurcay from the original tree.

## 

Discuss the ways a decision tree could manifest algorithmic bias.  

A decision tree could manifest algorithmic bias if the training data and testing data are not representative of each other. If this is the case, the model will predict wrongly and make force some data into classifications they do not belong in. 